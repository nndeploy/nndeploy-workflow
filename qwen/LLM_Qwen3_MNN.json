{
    "key_": "nndeploy.dag.Graph",
    "name_": "LLM_Qwen3_MNN",
    "developer_": "always",
    "source_": "https://github.com/QwenLM/Qwen",
    "desc_": "Qwen Serial LLM Workflow impl MNN",
    "device_type_": "kDeviceTypeCodeCpu:0",
    "version_": "1.0.0",
    "required_params_": [],
    "ui_params_": [],
    "io_params_": [],
    "dropdown_params_": {},
    "is_dynamic_input_": false,
    "inputs_": [],
    "is_dynamic_output_": false,
    "outputs_": [],
    "is_graph_": true,
    "parallel_type_": "kParallelTypeNone",
    "is_inner_": false,
    "node_type_": "Intermediate",
    "is_time_profile_": false,
    "is_debug_": false,
    "is_external_stream_": false,
    "is_graph_node_share_stream_": true,
    "queue_max_size_": 16,
    "is_loop_max_flag_": true,
    "loop_count_": -1,
    "unused_node_names_": [],
    "image_url_": [
        "template[http,modelscope]@https://template.cn/template.jpg"
    ],
    "video_url_": [
        "template[http,modelscope]@https://template.cn/template.mp4"
    ],
    "audio_url_": [
        "template[http,modelscope]@https://template.cn/template.mp3"
    ],
    "model_url_": [
        "modelscope@nndeploy/nndeploy:qwen/Qwen3-0.6B/mnn/llm.mnn",
        "modelscope@nndeploy/nndeploy:qwen/Qwen3-0.6B/mnn/llm.mnn.weight",
        "modelscope@nndeploy/nndeploy:qwen/Qwen3-0.6B/mnn/llm.mnn.json",
        "modelscope@nndeploy/nndeploy:qwen/Qwen3-0.6B/mnn/config.json",
        "modelscope@nndeploy/nndeploy:qwen/Qwen3-0.6B/mnn/llm_config.json",
        "modelscope@nndeploy/nndeploy:qwen/Qwen3-0.6B/mnn/tokenizer.txt",
        "modelscope@nndeploy/nndeploy:qwen/Qwen3-0.6B/tokenizer.json"
    ],
    "other_url_": [
        "template[http,modelscope]@https://template.cn/template.txt"
    ],
    "node_repository_": [
        {
            "key_": "nndeploy::llm::Prefill",
            "name_": "Prefill_1",
            "developer_": "",
            "source_": "",
            "desc_": "Prefill: Prefill pipeline",
            "device_type_": "kDeviceTypeCodeCpu:0",
            "version_": "1.0.0",
            "required_params_": [],
            "ui_params_": [],
            "io_params_": [],
            "dropdown_params_": {},
            "is_dynamic_input_": false,
            "inputs_": [
                {
                    "type_": "TokenizerText",
                    "name_": "Prompt_4@output_0",
                    "desc_": "input_0"
                }
            ],
            "is_dynamic_output_": false,
            "outputs_": [
                {
                    "type_": "TokenizerIds",
                    "name_": "Prefill_1@prefill_sampler@sampled_token",
                    "desc_": "sampled_token"
                }
            ],
            "is_graph_": true,
            "parallel_type_": "kParallelTypeNone",
            "is_inner_": true,
            "node_type_": "Intermediate",
            "image_url_": [
                "template[http,modelscope]@https://template.cn/template.jpg"
            ],
            "video_url_": [
                "template[http,modelscope]@https://template.cn/template.mp4"
            ],
            "audio_url_": [
                "template[http,modelscope]@https://template.cn/template.mp3"
            ],
            "model_url_": [
                "template[http,modelscope]@https://template.cn/template.onnx"
            ],
            "other_url_": [
                "template[http,modelscope]@https://template.cn/template.txt"
            ],
            "size": {
                "width": 200,
                "height": 80
            },
            "node_repository_": [
                {
                    "key_": "nndeploy::tokenizer::TokenizerEncodeCpp",
                    "name_": "tokenizer_encode",
                    "developer_": "",
                    "source_": "",
                    "desc_": "A tokenizer encode node that uses the C++ tokenizers library to encode text into token IDs. Supports HuggingFace and BPE tokenizers. Can encode single strings or batches of text. Provides vocabulary lookup and token-to-ID conversion.",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "version_": "1.0.0",
                    "required_params_": [],
                    "ui_params_": [],
                    "io_params_": [],
                    "dropdown_params_": {},
                    "is_dynamic_input_": false,
                    "inputs_": [
                        {
                            "desc_": "input_0",
                            "name_": "Prompt_4@output_0",
                            "type_": "TokenizerText"
                        }
                    ],
                    "is_dynamic_output_": false,
                    "outputs_": [
                        {
                            "desc_": "output_0",
                            "name_": "Prefill_1@tokenizer_encode@output_0",
                            "type_": "TokenizerIds"
                        }
                    ],
                    "node_type_": "Intermediate",
                    "param_": {
                        "required_params_": [
                            "tokenizer_type_"
                        ],
                        "ui_params_": [],
                        "io_params_": [],
                        "dropdown_params_": {},
                        "is_path_": true,
                        "tokenizer_type_": "kTokenizerTypeHF",
                        "json_blob_": "resources/models/qwen/Qwen3-0.6B/tokenizer.json",
                        "model_blob_": "",
                        "vocab_blob_": "",
                        "merges_blob_": "",
                        "added_tokens_": "",
                        "max_length_": 77
                    },
                    "node_repository_": []
                },
                {
                    "key_": "nndeploy::llm::LlmInfer",
                    "name_": "prefill_infer",
                    "developer_": "",
                    "source_": "",
                    "desc_": "LlmInfer: LLM inference CompositeNode",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "version_": "1.0.0",
                    "required_params_": [],
                    "ui_params_": [],
                    "io_params_": [],
                    "dropdown_params_": {},
                    "is_dynamic_input_": true,
                    "inputs_": [
                        {
                            "desc_": "input_tokens",
                            "name_": "Prefill_1@tokenizer_encode@output_0",
                            "type_": "TokenizerIds"
                        }
                    ],
                    "is_dynamic_output_": false,
                    "outputs_": [
                        {
                            "desc_": "output_logits",
                            "name_": "Prefill_1@prefill_infer@output_logits",
                            "type_": "Tensor"
                        }
                    ],
                    "is_composite_node_": true,
                    "node_type_": "Intermediate",
                    "is_prefill": true,
                    "model_key": "Qwen",
                    "infer_key": "MnnLlmInfer",
                    "config_path": [
                        "resources/models/qwen/Qwen3-0.6B/mnn/config.json"
                    ],
                    "model_inputs": [
                        "input_ids",
                        "attention_mask",
                        "position_ids",
                        "past_key_values"
                    ],
                    "model_outputs": [
                        "logits",
                        "presents"
                    ],
                    "node_repository_": []
                },
                {
                    "key_": "nndeploy::llm::Sampler",
                    "name_": "prefill_sampler",
                    "developer_": "",
                    "source_": "",
                    "desc_": "Sample generates next token from model logits using various sampling strategies:\n1. Greedy sampling - select token with highest probability\n2. Temperature sampling - sample from temperature-scaled distribution\n3. Top-K sampling - sample from top K most likely tokens\n4. Top-P (nucleus) sampling - sample from tokens with cumulative probability <= P\n5. Min-P sampling - filter tokens below minimum probability threshold\n6. Repetition penalty - penalize repeated tokens/n-grams\n\nInputs:\n- inputs[0]: Tensor containing model logits for next token prediction\nOutputs:\n- outputs[0]: TokenizerIds containing sampled token ID\n",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "version_": "1.0.0",
                    "required_params_": [],
                    "ui_params_": [],
                    "io_params_": [],
                    "dropdown_params_": {},
                    "is_dynamic_input_": false,
                    "inputs_": [
                        {
                            "desc_": "logits",
                            "name_": "Prefill_1@prefill_infer@output_logits",
                            "type_": "Tensor"
                        }
                    ],
                    "is_dynamic_output_": false,
                    "outputs_": [
                        {
                            "desc_": "sampled_token",
                            "name_": "Prefill_1@prefill_sampler@sampled_token",
                            "type_": "TokenizerIds"
                        }
                    ],
                    "node_type_": "Intermediate",
                    "param_": {
                        "required_params_": [],
                        "ui_params_": [],
                        "io_params_": [],
                        "dropdown_params_": {
                            "sampler": [
                                "greedy",
                                "temperature",
                                "topK",
                                "topP",
                                "minP",
                                "tfs",
                                "typical",
                                "penalty",
                                "ngram"
                            ]
                        },
                        "sampler": "temperature",
                        "temperature": 0.800000011920929,
                        "topK": 40,
                        "topP": 0.8999999761581421,
                        "minP": 0.05000000074505806,
                        "tfsZ": 1,
                        "typical": 0.949999988079071,
                        "penalty": 1.0499999523162842,
                        "ngram": 8,
                        "ngram_factor": 1.0199999809265137,
                        "max_penalty": 10,
                        "mixed_samplers": [
                            "topK",
                            "tfs",
                            "typical",
                            "topP",
                            "minP",
                            "temperature"
                        ]
                    },
                    "node_repository_": []
                }
            ]
        },
        {
            "key_": "nndeploy::llm::LlmOut",
            "name_": "LlmOut_3",
            "developer_": "",
            "source_": "",
            "desc_": "Print TokenizerText content and save to temporary output file.",
            "device_type_": "kDeviceTypeCodeCpu:0",
            "version_": "1.0.0",
            "required_params_": [
                "path_"
            ],
            "ui_params_": [],
            "io_params_": [],
            "dropdown_params_": {},
            "is_dynamic_input_": false,
            "inputs_": [
                {
                    "desc_": "input_0",
                    "name_": "Decode_2@stream_out@output_text",
                    "type_": "TokenizerText"
                }
            ],
            "is_dynamic_output_": false,
            "outputs_": [],
            "node_type_": "Output",
            "io_type_": "Text",
            "path_": "resources/others/llm_out.txt",
            "size": {
                "width": 200,
                "height": 80
            },
            "node_repository_": []
        },
        {
            "key_": "nndeploy::llm::Prompt",
            "name_": "Prompt_4",
            "developer_": "",
            "source_": "",
            "desc_": "Generate TokenizerText from prompt string using optional template.",
            "device_type_": "kDeviceTypeCodeCpu:0",
            "version_": "1.0.0",
            "required_params_": [],
            "ui_params_": [],
            "io_params_": [],
            "dropdown_params_": {},
            "is_dynamic_input_": false,
            "inputs_": [],
            "is_dynamic_output_": false,
            "outputs_": [
                {
                    "desc_": "output_0",
                    "name_": "Prompt_4@output_0",
                    "type_": "TokenizerText"
                }
            ],
            "node_type_": "Input",
            "io_type_": "String",
            "param_": {
                "required_params_": [
                    "user_content_"
                ],
                "ui_params_": [],
                "io_params_": [],
                "dropdown_params_": {},
                "prompt_template_": "<|im_start|>user\n%s<|im_end|>\n<|im_start|>assistant\n",
                "user_content_": "谁是李白"
            },
            "size": {
                "width": 200,
                "height": 80
            },
            "node_repository_": []
        },
        {
            "key_": "nndeploy::llm::Decode",
            "name_": "Decode_2",
            "developer_": "",
            "source_": "",
            "desc_": "Decode: Decode pipeline",
            "device_type_": "kDeviceTypeCodeCpu:0",
            "version_": "1.0.0",
            "required_params_": [],
            "ui_params_": [],
            "io_params_": [],
            "dropdown_params_": {},
            "is_dynamic_input_": false,
            "inputs_": [
                {
                    "type_": "TokenizerIds",
                    "name_": "Prefill_1@prefill_sampler@sampled_token",
                    "desc_": "input_tokens"
                }
            ],
            "is_dynamic_output_": false,
            "outputs_": [
                {
                    "type_": "TokenizerText",
                    "name_": "Decode_2@stream_out@output_text",
                    "desc_": "output_text"
                }
            ],
            "is_graph_": true,
            "parallel_type_": "kParallelTypeNone",
            "is_inner_": true,
            "is_loop_": true,
            "node_type_": "Intermediate",
            "image_url_": [
                "template[http,modelscope]@https://template.cn/template.jpg"
            ],
            "video_url_": [
                "template[http,modelscope]@https://template.cn/template.mp4"
            ],
            "audio_url_": [
                "template[http,modelscope]@https://template.cn/template.mp3"
            ],
            "model_url_": [
                "template[http,modelscope]@https://template.cn/template.onnx"
            ],
            "other_url_": [
                "template[http,modelscope]@https://template.cn/template.txt"
            ],
            "tokenizer_txt_": "",
            "stop_texts_": [
                "<|endoftext|>",
                "<|im_end|>",
                "</s>",
                "<|end|>",
                "<|eot_id|>",
                "[DONE]"
            ],
            "size": {
                "width": 200,
                "height": 80
            },
            "node_repository_": [
                {
                    "key_": "nndeploy::llm::LlmInfer",
                    "name_": "decode_infer",
                    "developer_": "",
                    "source_": "",
                    "desc_": "LlmInfer: LLM inference CompositeNode",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "version_": "1.0.0",
                    "required_params_": [],
                    "ui_params_": [],
                    "io_params_": [],
                    "dropdown_params_": {},
                    "is_dynamic_input_": true,
                    "inputs_": [
                        {
                            "desc_": "input_tokens",
                            "name_": "Prefill_1@prefill_sampler@sampled_token",
                            "type_": "TokenizerIds"
                        }
                    ],
                    "is_dynamic_output_": false,
                    "outputs_": [
                        {
                            "desc_": "output_logits",
                            "name_": "Decode_2@decode_infer@output_logits",
                            "type_": "Tensor"
                        }
                    ],
                    "is_composite_node_": true,
                    "node_type_": "Intermediate",
                    "is_prefill": false,
                    "model_key": "Qwen",
                    "infer_key": "MnnLlmInfer",
                    "config_path": [
                        "resources/models/qwen/Qwen3-0.6B/mnn/config.json"
                    ],
                    "model_inputs": [
                        "input_ids",
                        "attention_mask",
                        "position_ids",
                        "past_key_values"
                    ],
                    "model_outputs": [
                        "logits",
                        "presents"
                    ],
                    "node_repository_": []
                },
                {
                    "key_": "nndeploy::llm::Sampler",
                    "name_": "decode_sampler",
                    "developer_": "",
                    "source_": "",
                    "desc_": "Sample generates next token from model logits using various sampling strategies:\n1. Greedy sampling - select token with highest probability\n2. Temperature sampling - sample from temperature-scaled distribution\n3. Top-K sampling - sample from top K most likely tokens\n4. Top-P (nucleus) sampling - sample from tokens with cumulative probability <= P\n5. Min-P sampling - filter tokens below minimum probability threshold\n6. Repetition penalty - penalize repeated tokens/n-grams\n\nInputs:\n- inputs[0]: Tensor containing model logits for next token prediction\nOutputs:\n- outputs[0]: TokenizerIds containing sampled token ID\n",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "version_": "1.0.0",
                    "required_params_": [],
                    "ui_params_": [],
                    "io_params_": [],
                    "dropdown_params_": {},
                    "is_dynamic_input_": false,
                    "inputs_": [
                        {
                            "desc_": "logits",
                            "name_": "Decode_2@decode_infer@output_logits",
                            "type_": "Tensor"
                        }
                    ],
                    "is_dynamic_output_": false,
                    "outputs_": [
                        {
                            "desc_": "sampled_token",
                            "name_": "Decode_2@decode_sampler@sampled_token",
                            "type_": "TokenizerIds"
                        }
                    ],
                    "node_type_": "Intermediate",
                    "param_": {
                        "required_params_": [],
                        "ui_params_": [],
                        "io_params_": [],
                        "dropdown_params_": {
                            "sampler": [
                                "greedy",
                                "temperature",
                                "topK",
                                "topP",
                                "minP",
                                "tfs",
                                "typical",
                                "penalty",
                                "ngram"
                            ]
                        },
                        "sampler": "temperature",
                        "temperature": 0.800000011920929,
                        "topK": 40,
                        "topP": 0.8999999761581421,
                        "minP": 0.05000000074505806,
                        "tfsZ": 1,
                        "typical": 0.949999988079071,
                        "penalty": 1.0499999523162842,
                        "ngram": 8,
                        "ngram_factor": 1.0199999809265137,
                        "max_penalty": 10,
                        "mixed_samplers": [
                            "topK",
                            "tfs",
                            "typical",
                            "topP",
                            "minP",
                            "temperature"
                        ]
                    },
                    "node_repository_": []
                },
                {
                    "key_": "nndeploy::tokenizer::TokenizerDecodeCpp",
                    "name_": "tokenizer_decode",
                    "developer_": "",
                    "source_": "",
                    "desc_": "A tokenizer decode node that uses the C++ tokenizers library to decode token IDs into text. Supports HuggingFace and BPE tokenizers. Can decode single token IDs or batches of token IDs. Provides token-to-text conversion.",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "version_": "1.0.0",
                    "required_params_": [],
                    "ui_params_": [],
                    "io_params_": [],
                    "dropdown_params_": {},
                    "is_dynamic_input_": false,
                    "inputs_": [
                        {
                            "desc_": "input_0",
                            "name_": "Decode_2@decode_sampler@sampled_token",
                            "type_": "TokenizerIds"
                        }
                    ],
                    "is_dynamic_output_": false,
                    "outputs_": [
                        {
                            "desc_": "output_0",
                            "name_": "Decode_2@tokenizer_decode@output_0",
                            "type_": "TokenizerText"
                        }
                    ],
                    "node_type_": "Intermediate",
                    "param_": {
                        "required_params_": [
                            "tokenizer_type_"
                        ],
                        "ui_params_": [],
                        "io_params_": [],
                        "dropdown_params_": {},
                        "is_path_": true,
                        "tokenizer_type_": "kTokenizerTypeHF",
                        "json_blob_": "resources/models/qwen/Qwen3-0.6B/tokenizer.json",
                        "model_blob_": "",
                        "vocab_blob_": "",
                        "merges_blob_": "",
                        "added_tokens_": "",
                        "max_length_": 77
                    },
                    "node_repository_": []
                },
                {
                    "key_": "nndeploy::llm::StreamOut",
                    "name_": "stream_out",
                    "developer_": "",
                    "source_": "",
                    "desc_": "StreamOut: Stream output node",
                    "device_type_": "kDeviceTypeCodeCpu:0",
                    "version_": "1.0.0",
                    "required_params_": [],
                    "ui_params_": [],
                    "io_params_": [],
                    "dropdown_params_": {},
                    "is_dynamic_input_": false,
                    "inputs_": [
                        {
                            "desc_": "input_text",
                            "name_": "Decode_2@tokenizer_decode@output_0",
                            "type_": "TokenizerText"
                        }
                    ],
                    "is_dynamic_output_": false,
                    "outputs_": [
                        {
                            "desc_": "output_text",
                            "name_": "Decode_2@stream_out@output_text",
                            "type_": "TokenizerText"
                        }
                    ],
                    "node_type_": "Intermediate",
                    "enable_stream_": true,
                    "node_repository_": []
                }
            ]
        }
    ],
    "nndeploy_ui_layout": {
        "layout": {
            "Prefill_1": {
                "position": {
                    "x": -637,
                    "y": 233.55
                },
                "size": {
                    "width": 200,
                    "height": 80
                },
                "expanded": true,
                "children": {
                    "tokenizer_encode": {
                        "position": {
                            "x": 100,
                            "y": 0
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    },
                    "prefill_infer": {
                        "position": {
                            "x": 100,
                            "y": 182.45
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    },
                    "prefill_sampler": {
                        "position": {
                            "x": 100,
                            "y": 383
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    }
                }
            },
            "LlmOut_3": {
                "position": {
                    "x": 514,
                    "y": 47.05000000000001
                },
                "size": {
                    "width": 200,
                    "height": 80
                },
                "expanded": true
            },
            "Prompt_4": {
                "position": {
                    "x": -942,
                    "y": 68.55000000000001
                },
                "size": {
                    "width": 200,
                    "height": 80
                },
                "expanded": true
            },
            "Decode_2": {
                "position": {
                    "x": -218,
                    "y": 236.05
                },
                "size": {
                    "width": 200,
                    "height": 80
                },
                "expanded": true,
                "children": {
                    "decode_infer": {
                        "position": {
                            "x": 100,
                            "y": 0
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    },
                    "decode_sampler": {
                        "position": {
                            "x": 100,
                            "y": 195.59999999999997
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    },
                    "tokenizer_decode": {
                        "position": {
                            "x": 100,
                            "y": 378
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    },
                    "stream_out": {
                        "position": {
                            "x": 365,
                            "y": 0
                        },
                        "size": {
                            "width": 200,
                            "height": 80
                        },
                        "expanded": true
                    }
                }
            }
        },
        "groups": []
    }
}